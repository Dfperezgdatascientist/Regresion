---
title: "Regresión"
author: "*Daniel Felipe Pérez Grajales* <br/> *Universidad Nacional de Colombia - Sede Medellín* <br/><br/> *Efraín Galvis Amaya* <br/> *Universidad Nacional de Colombia - Sede Medellín* <br/> <br/> **Profesor**: *Juan David Ospina Arango* <br/> *Universidad Nacional de Colombia - Sede Medellín* <br/> *Departamento de Ciencias de la Computación y de la Decisión* <br/> *Decisiones bajo incertidumbre (Optimización para aprendizaje de máquina)*"
date: "28 de mayo de 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(readxl)
library(tidyr)
library(tidyverse)
library(ggthemes)
library(Hmisc)
library(caret)
library(DT)
library(glmnet)
library(readr)# for fast reading of input files
library(mice)      # mice package for Multivariate Imputation by 
library(keras)     # for neural nets
library(corrplot)  # for correlation
library(progress)
library(GGally)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(MASS)
library(ISLR)
library(xgboost)  
library(pROC)
library(e1071)

options(warn=-1)###
BD_REV <- read_excel("Data/Real estate valuation data set.xlsx")
BD_REV <- BD_REV[,-1]
colnames(BD_REV) <- c("x1","x2","x3","x4","x5","x6","y")

```


```{r , echo=FALSE}
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))
  
  
  # Model performance metrics
  data.frame(
    RMSE = RMSE,
    Rsquare = R_square
  )
  
}
```

## Predicción del precio de la vivienda en Taiwan

Utilizando el conjunto de datos Real estate valuation data set Data Set lleve a un ejercicio del predicción del precio de la vivienda en Taiwan utilizando los siguientes métodos de predicción:

* Regresión lineal (clásica y elastic net)

* Ensambles de árboles: bosques aleatorios y XGBoost

* Máquinas de soporte vectorial

* Redes neuronales

Es posible dividir el conjunto de datos en entrenamiento y validación o uzar un esquema de validación cruzada.

El conjunto de datos se referncia en la siguiente cita:
Citación: Yeh, I. C., & Hsu, T. K. (2018). Building real estate valuation models with comparative approach through case-based reasoning. Applied Soft Computing, 65, 260-271.
La definición de las variables es la siguiente:

+ X1: fecha de la transacción (por ejemplo 2013.250=2013 Marzo, * 2013.500=2013 Junio, etc.)

+ X2: edad de la casa en años

+ X3: distancial al MRT (transporte masivo) más cercano en metros

+ X4: número de tiendas de conveniencia en el vecindario (entero)

+ X5: latitud (unidad: grados)

+ X6: longitude (unidad: grados)

+ Y: precio por unidad de área (10000 Nuevos dólares taiwaneses/ 3.3 \(m^2\))

### Data

```{r , echo=FALSE}

datatable(head(BD_REV))

```

### Descriptivo general

```{r , echo=FALSE}
paste("Identificación de NA'S por variable")

apply(BD_REV, 2, function(x){sum(is.na(x))})/100 # no hay NA's

paste("Descriptivo de variables1")

summary(BD_REV)

paste("Descriptivo de variables2")

describe(BD_REV)

```

### Análisis de correlación de variables

```{r , echo=FALSE}
BD_REV %>%
  gather(x, y, x2:x4) %>%
  ggplot(aes(x = y)) +
  facet_wrap(~ x, ncol = 3, scales = "free") +
  geom_density(alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "top") +
  scale_color_tableau() +
  scale_fill_tableau()

par(mfrow=c(2,2))
plot(BD_REV$x2, BD_REV$y)
plot(BD_REV$x3, BD_REV$y)
plot(BD_REV$x4, BD_REV$y)
par(mfrow=c(1,1))
grid()
```

```{r , echo=FALSE}
# Analisis de correlacion ####
GGally::ggpairs(head(BD_REV[, -c(1)], lower = list(continuous = "smooth"),
        diag = list(continuous = "barDiag"), axisLabels = "none"),
        lower = list(continuous = wrap("cor", alpha = 1,size=3), 
                     combo = "box"),
        upper = list(continuous = wrap("smooth", alpha = 1, size=1, color='blue')),progress=F)+
  theme(axis.text = element_text(size = 6))
#grid()
```

Del análisis preliminar se pueden extraer las siguientes conclusiones:

* La variable que tienen una mayor relación lineal con el precio por unidad de área: x5: Lat y x6: Long es decir la ubicación del inmueble (r= 0.823 y r= 0.760).

* La variable que tienen medianamente una relación lineal con el precio por unidad de área: X3:distancial al MRT (transporte masivo) más cercano en metros (r= -0.507).

* X3:distancial al MRT (transporte masivo) más cercano en metros y X4: número de tiendas de conveniencia en el vecindario (entero) están medianamente correlacionados (r = -0.729) y existe una dependencia lineal por lo que posiblemente no sea útil introducir ambos predictores en los modelos.

* X2: edad de la casa en años y X4:número de tiendas de conveniencia en el vecindario (entero) están correlacionados (r = 0.894) y existe una dependencia lineal por lo que posiblemente no sea útil introducir ambos predictores en los modelos.

* La variable X3: distancial al MRT (transporte masivo) más cercano en metros se debería hacer una transformación que posiblemente haría más normal su distribución.


## se crean la variable año

```{r , echo=FALSE}
BD_REV$ano <- as.numeric(substr(BD_REV$x1, 1,4))
table(BD_REV$ano)
```

# Modelamiento 

escalamiento de las variables para transformarlas

```{r , echo=FALSE}

BD_REV <- as.data.frame(scale(BD_REV[,-1]))
datatable(head(BD_REV))

```


### Regresión lineal (clásica)

```{r , echo=FALSE}
#LM ####
step(lm(y ~ ., data = BD_REV), direction = "both")
summary(m1 <- lm(formula = y ~ x2 + x3 + x4 + x5 + ano, data = BD_REV))
```

se puede observar que despues de aplicar la la busqueda del mejor modelo con la función step, 
nos indica que el mejor modelo tiene un Rcuadrado de 58 % y 
```{r , echo=FALSE}
x <- data.frame(summary(m1)[4])[4]
x$names <- rownames(x)
paste("la variable mas significativa es: ", x[order(x$coefficients.Pr...t..,decreasing = F),][1,2])
```

Calculando RMSE para la regresión

el modelo de regresión Lineal tiene un error cuadrático medio de:
```{r , echo=FALSE}
library(hydroGOF)
predY <- predict (m1, BD_REV)
paste(round((rmse_ml=rmse(predY,BD_REV$y))*100,2),"%",sep = "")
```
es decir, el modelo es capaz de predecir con una precisión del 
```{r , echo=FALSE}
paste(100-round(rmse_ml*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.

### Regresión lineal (elastic net)

```{r , echo=FALSE}
# which combines the ridge and lasso
set.seed(12)
indexes = createDataPartition(BD_REV$y, p = .80, list = F)
train = BD_REV[indexes, ]
test = BD_REV[-indexes, ]

Y_train <- train %>% dplyr::select(y) %>% as.matrix()
X_train <- train %>% dplyr::select(-y) %>% as.matrix()

Y_test <- test %>% dplyr::select(y) %>% as.matrix()
X_test <- test %>% dplyr::select(-y) %>% as.matrix()

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           search = "random",
                           verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                     data = (train),
                     method = "glmnet",
                     preProcess = c("center", "scale"),
                     tuneLength = 10,
                     trControl = train_cont)
```
Mejor parámetros del modelo

```{r , echo=FALSE}
# Best tuning parameter
elastic_reg$bestTune
```

el modelo de regresión elastic net tiene error cuadrático medio de 

```{r , echo=FALSE}
# Make predictions on training set
predictions_train <- predict(elastic_reg, test)

rmse_elastic = eval_results(test$y, predictions_train, test)
paste(round((rmse_elastic = as.numeric(rmse_elastic[1]))*100,2),"%",sep = "")


```
es decir, el modelo es capaz de predecir con una precisión del
```{r , echo=FALSE}
paste(100-round((rmse_elastic)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.

### Ensambles de árboles

bosques aleatorios 

```{r , echo=FALSE}
# Random Forest ####
set.seed(12)
indexes = createDataPartition(BD_REV$y, p = .80, list = F)
train = BD_REV[indexes, ]
test = BD_REV[-indexes, ]

(forest = randomForest(y ~ ., data = train, mtry = 3, importance = TRUE, ntrees = 500))
importance(forest, type = 1)
```

se concluye que: la variable mas importante es X3:distancia al MRT (transporte masivo) más cercano en metros seguido de la X5: Latitud, es decir posición de la ubicación del inmueble suceptible al norte o sur de taiwan para predecir el precio por unidad de área de la vivienda en Taiwan.

```{r , echo=FALSE}
varImpPlot(forest, type = 1)

# plot(forest_tst_pred, df_tst$y,
#      xlab = "Predicted", ylab = "Actual",
#      main = "Predicted vs Actual: Random Forest, Test Data",
#      col = "dodgerblue", pch = 20)
# grid()
# abline(0, 1, col = "darkorange", lwd = 2)

```


el modelo de bosques aleatorios tiene un error cuadrático medio de 

```{r , echo=FALSE}
forest_trn_pred = predict(forest, newdata = test)
paste(round((forest_rmse = calc_rmse(forest_trn_pred, test$y))*100,2),"%",sep = "")
#forest_oob_rmse = calc_rmse(forest$predicted, test$y)
```
es decir, el modelo es capaz de predecir con una precisión del
```{r , echo=FALSE}
paste(100-round((forest_rmse)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.

### XGBoost

```{r , echo=FALSE}

# XGboost ####
set.seed(12)
indexes = createDataPartition(BD_REV$y, p = .80, list = F)
train = BD_REV[indexes, ]
test = BD_REV[-indexes, ]

train_x = data.matrix(train[, -6])
train_y = train[,6]

test_x = data.matrix(test[, -6])
test_y = test[, 6]

xgb_train = xgboost::xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgboost::xgb.DMatrix(data = test_x, label = test_y)

xgbc = xgboost::xgboost(data = xgb_train, max.depth = 2, nrounds = 50)
print(xgbc)
```


```{r , echo=FALSE}
pred_y = predict(xgbc, xgb_test)

mse = mean((test_y - pred_y)^2)
mae = caret::MAE(test_y, pred_y)
rmseXG = caret::RMSE(test_y, pred_y)

cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmseXG)
#MSE:  0.2468287 MAE:  0.3547412  RMSE:  0.4968186
x = 1:length(test_y)
plot(x, test_y, col = "red", type = "l")
lines(x, pred_y, col = "blue", type = "l")
legend(x = 1, y = 3,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))

```


el modelo XGBoost tiene error cuadrático medio de

```{r , echo=FALSE}
paste(round((rmseXG)*100,2),"%",sep = "")
```
es decir, el modelo es capaz de predecir con una precisión del
```{r , echo=FALSE}
paste(100-round((rmseXG)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.

### Máquinas de soporte vectorial

```{r , echo=FALSE}
set.seed(1234)
indexes = createDataPartition(BD_REV$y, p = .80, list = F)
train = BD_REV[indexes, ]
test = BD_REV[-indexes, ]

train_y = train[,6]
test_y = test[, 6]

svm <- svm(y ~ ., data = train,type = 'eps-regression')
pred_valid_svm <- predict(svm, newdata = test)


#(RMSEsvm=rmse(pred_valid_svm,test$y))

```

el modelo de máquinas de soporte vectorial tiene error cuadrático medio de 
```{r , echo=FALSE}
paste(round((RMSEsvm=rmse(pred_valid_svm,test$y))*100,2),"%",sep = "")
```
es decir, el modelo es capaz de predecir con una precisión del
```{r , echo=FALSE}
paste(100-round((RMSEsvm)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.


### Redes neuronales

```{r , echo=FALSE}
# Red neuronal ####
library(nnet)

set.seed(199)
nnet <- nnet(y ~ ., data=train, size=10, linout=TRUE, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)

nnetpred <- predict(nnet, newdata = test)

```

el modelo de Redes neuronales tiene error cuadrático medio de
```{r , echo=FALSE}
paste(round((nnetrsme <- calc_rmse(nnetpred, test$y))*100,2),"%",sep = "")
```
es decir, el modelo es capaz de predecir con una precisión del
```{r , echo=FALSE}
paste(100-round((nnetrsme)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.


## Preguntas adicionales

*1. ¿Qué variables tienen el mayor impacto en el precio de la vivienda?*

Las variables de la ubicación del inmueble x5:Lat y x6:Long en el análisis de correlación se evidencia una dependecia lineal fuerte al precio por unidad de área de la vivienda en Taiwan.

Las variables X3:distancia al MRT (transporte masivo) más cercano en metros en el análisis de correlación se evidencia una dependecia lineal medianamente al precio por unidad de área de la vivienda en Taiwan.

*¿Cómo aporta cada modelo al conocimiento de este impacto?*

En el ajuste de los modelos, para la regresión lineal es la variable más significativa es X3:distancia al MRT (transporte masivo) más cercano en metros, al igual que el modelo bosques aleatorios, adicionalmente este modelo considera que la posición del inmueble en sentido sur o norte x5: Lat,es influyente en el precio por unidad de área de la vivienda en Taiwan.

```{r , echo=FALSE}
bd <- BD_REV[, c("x5", "x6","y")]

#as.numeric(quantile(bd$y))

bd$seg_precio <- cut(bd$y, 
                     breaks = as.numeric(quantile(bd$y)), 
                     labels = c("precio bajo", "precio medio", "precio alto", "precio muy alto"), 
                     include.lowest = T)
#table(bd$seg_precio, useNA = "always")

library(ggplot2)
ggplot(tidyr::drop_na(bd), aes(x=x6, y=x5, group=seg_precio)) +
  geom_point(aes(shape=seg_precio, color=seg_precio), size = 2.5)+
  theme_minimal()

```



*2. ¿Cuál es el mejor modelo entre los usados para resolver este problema?*

El modelo de bosques aleatorios que tiene un error cuadrático medio de
```{r , echo=FALSE}
paste(round((forest_rmse)*100,2),"%",sep = "")

```
es decir, es capaz de predecir con una precisión del 
```{r , echo=FALSE}
paste(100-round((forest_rmse)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan y fue el modelo con el mejor ajuste.

*¿Qué criterios se pueden utilizar para responder a esta pregunta?*

para este caso por los modelos de: Regresión lineal (clásica y elastic net),Ensambles de árboles: bosques aleatorios y XGBoost,Máquinas de soporte vectorial, Redes neuronales, se busca una medida del error de ajuste de la predicción que se pueda calcular en todos los modelos como lo es el error cuadrático medio(RMSE) y se pueda comparar entre todos.El de menor RMSE será el que tiene mejor ajuste al predecir el precio por unidad de área de la vivienda en Taiwan.

```{r , echo=FALSE}
# conclusiones ####

tabla_final <- data.frame(rmse_ml,
                          rmse_elastic,
                          forest_rmse,
                          rmseXG,
                          RMSEsvm,
                          nnetrsme)

tabla_final

```

por lo que el modelo de bosques aleatorios que tiene un error cuadrático medio más bajo de
```{r , echo=FALSE}
paste(round((forest_rmse)*100,2),"%",sep = "")
```
es decir, es capaz de predecir con una precisión del
```{r , echo=FALSE}
paste(100-round((forest_rmse)*100,2),"%",sep = "")
```
el precio por unidad de área de la vivienda en Taiwan.